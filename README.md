# llm_router_services

## âœ¨ Overview

`llm_router_services` provides **HTTP services** that implement the core functionality used by the LLMâ€‘Routerâ€™s plugin
system.  
The services expose guardrail and masking capabilities through Flask applications
that can be called by the corresponding plugins in `llm_router_plugins`.

Key components:

| Subâ€‘package              | Primary purpose                                                                                                                                                                                                                    |
|--------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **guardrails/**          | Hosts the NASKâ€‘PIB guardrail service (`nask_pib_guard_app.py`). It receives a JSON payload, chunks the text, runs a Huggingâ€‘Face classification pipeline, and returns a safety verdict (`safe` flag + detailed perâ€‘chunk results). |
| **maskers/**             | Contains the **BANonymizer** (`banonymizer.py` -- **under development**) â€“ a lightweight Flask service that performs tokenâ€‘classification based anonymisation of input text.                                                       |
| **run_*.sh** scripts     | Convenience wrappers to start the services (Gunicorn for the guardrail, plain Flask for the anonymiser).                                                                                                                           |
| **requirementsâ€‘gpu.txt** | Lists heavy dependencies (e.g., `transformers`) required for GPUâ€‘accelerated inference.                                                                                                                                            |

The services are **stateless**; they load their models once at startâ€‘up and then serve requests over HTTP.

---

## ğŸ›¡ï¸ Guardrails

Full documentation for the guardrails subâ€‘package is available
in [guardrail-readme](llm_router_services/guardrails/README.md).

The **guardrail** subâ€‘package implements safetyâ€‘checking services that can be queried via HTTP:

| Service                  | Model                               | Endpoint                           | Description                                                                                                                                    |
|--------------------------|-------------------------------------|------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------|
| **NASKâ€‘PIB Guard**       | `NASKâ€‘PIB/HerBERTâ€‘PLâ€‘Guard`         | `POST /api/guardrails/nask_guard`  | Polishâ€‘language safety classifier detecting unsafe content (e.g., hate, violence). Returns a `safe` flag and perâ€‘chunk classification details. |
| **Sojka Guard**          | `speakleash/Bielikâ€‘Guardâ€‘0.1Bâ€‘v1.0` | `POST /api/guardrails/sojka_guard` | Multiâ€‘category Polish safety model (HATE, VULGAR, SEX, CRIME, SELFâ€‘HARM). Returns detailed scores per category and an overall `safe` flag.     |
| **BANonymizer** (masker) | **under development**               | `POST /api/maskers/banonymizer`    | Tokenâ€‘classification based anonymiser that redacts personal data from input text.                                                              |

### How to use

1. **Start the service** â€“ run the provided shell script (`run_*_guardrail.sh` or `run_*_masker.sh`) or invoke the Flask
   module directly (e.g., `python -m llm_router_services.guardrails.speakleash.sojka_guard_app`).
2. **Send a JSON payload** â€“ the request body must be a JSON object; any string fields longer than 8 characters are
   extracted and classified.
3. **Interpret the response** â€“ the topâ€‘level `safe` boolean indicates the overall verdict, while `detailed` provides
   perâ€‘chunk (or perâ€‘category) results with confidence scores.

### Configuration

All guardrail services read configuration from environment variables prefixed with:

* `LLM_ROUTER_NASK_PIB_GUARD_` â€“ for the NASKâ€‘PIB guardrail.
* `LLM_ROUTER_SOJKA_GUARD_` â€“ for the Sojka guardrail.
* `LLM_ROUTER_BANONYMIZER_` â€“ for the masker.

Key variables include:

* `MODEL_PATH` â€“ path or Huggingâ€‘Face hub identifier of the model.
* `DEVICE` â€“ `-1` for CPU or CUDA device index for GPU inference.
* `FLASK_HOST` / `FLASK_PORT` â€“ network binding for the Flask server.

### Extensibility

The guardrail architecture is built around the **`GuardrailBase`** abstract class and a **factory** (
`GuardrailClassifierModelFactory`). To add a new safety model:

1. Implement a concrete subclass of `GuardrailBase` (or reuse `TextClassificationGuardrail`).
2. Provide a `GuardrailModelConfig` implementation with modelâ€‘specific thresholds.
3. Register the model type in the factory if a new identifier is required.

---

## ğŸ“œ License

See the [LICENSE](LICENSE) file.

---

*Happy masking and safe routing!*
